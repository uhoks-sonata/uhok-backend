# [완료] 추천 ML 서비스 분리 전략

> **문서 요약**: 이 문서는 레시피 추천 기능 중 ML 모델 추론 부분을 별도의 서비스(`uhok-ml-inference`)로 분리하기 위한 초기 계획을 다룹니다.
>
> **최종 결정**: 논의 결과, **전략 B (Remote Vector Store)** 가 채택되어 현재 시스템에 구현되었습니다.

---

## 분리 전략 (A/B)

### 1. 전략 A – Remote Embed + DB pgvector 유지
- **내용**: ML 서버는 임베딩만 생성하고(`POST /v1/embed`), 백엔드가 이 임베딩 값으로 직접 PostgreSQL(pgvector)에 유사도 검색 쿼리(`<->`)를 실행하는 방식.
- **장점**: 기존 DB와 인덱스 구조를 그대로 활용할 수 있어 데이터 이관 부담이 적음.
- **단점**: 백엔드 서비스가 벡터 DB의 쿼리 문법에 계속 의존하게 됨.

### 2. 전략 B – Remote Vector Store (채택)
- **내용**: ML 서버가 쿼리 텍스트를 받아 임베딩 생성부터 벡터 검색까지 모두 수행한 후, 최종 결과인 **Top-K 레시피 ID 목록**을 반환하는 방식. (`POST /v1/search`)
- **장점**:
    - 백엔드는 벡터 검색 로직으로부터 완전히 분리됨 (API 호출만 하면 됨).
    - ML 서버의 검색 성능을 독립적으로 최적화하거나, 나중에 FAISS 같은 다른 벡터 엔진으로 교체하기 용이함.
- **단점**: ML 서버가 자체적으로 벡터 인덱스를 관리해야 하므로, 데이터 동기화 파이프라인이 필요함.

---

## 단계별 진행 순서 (체크리스트)

- **[완료]** **0. 사전 정리**: **전략 B**로 결정 완료.
- **[완료]** **1. 새 리포 생성 (`uhok-ml-inference`)**: FastAPI 기반으로 기본 구조 생성 완료.
- **[완료]** **2. API 스펙 정의**: 전략 B에 따라 `/health`, `/api/v1/search` 등 최종 API 스펙 확정 및 구현 완료.
- **[완료]** **3. ML 서비스 구현**: `sentence-transformers` 모델 로딩 및 캐싱, API 엔드포인트 구현 완료.
- **[완료]** **4. 컨테이너화 & 실행**: `Dockerfile` 작성 및 컨테이너화 완료.
- **[완료]** **5. 인프라 배치**: 내부망(VPC)을 통해 서비스가 배포되고 헬스체크가 동작하도록 구성 완료.
- **[완료]** **6. 백엔드 연결**:
    - `.env`에 `ML_INFERENCE_URL` 등 관련 환경 변수 추가 완료.
    - `ports.py`와 `remote_ml_adapter.py`를 통해 원격 ML 서비스를 호출하는 어댑터 구현 완료.
    - `recipe_router.py`가 이 어댑터를 통해 ML 서비스와 연동되도록 수정 완료.
- **[완료]** **7. 데이터/인덱스 준비**: `uhok-ml-inference` 서비스가 자체적으로 PostgreSQL의 벡터 테이블(`RECIPE_VECTOR_TABLE`)에 접근하여 인덱스를 사용하도록 구성 완료.
- **[진행중]** **8. 점진적 전환**: 현재 API 라우터를 통해 신규 로직으로 트래픽이 전환됨.
- **[진행중]** **9. 테스트 & 검증**: 단위/통합 테스트 진행 중.
- **[진행중]** **10. 문서 & 운영**: API 계약 정의 및 이관 절차 문서화 진행 중 (이 문서가 그 일부임).

---

## 원격 어댑터 구현
- **요약**: 아래 설계안을 바탕으로 `remote_ml_adapter.py`에 실제 코드가 구현되었습니다.
- **참고**: 실제 최종 코드는 `services/recipe/utils/remote_ml_adapter.py` 파일의 `RemoteMLAdapter` 클래스를 참고하세요.